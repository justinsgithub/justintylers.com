---
title: "What Scraping 1.4 Million Records Actually Looks Like"
description: "The unglamorous reality of building a large-scale web scraping system. Rate limits, VPN rotation, broken HTML, and data cleaning."
publishedAt: "2026-02-20"
category: "tech"
tags: ["web-scraping", "data-engineering", "python", "behind-the-scenes"]
featured: false
draft: true
---

When I tell people I built a platform with 1.4 million contractor leads, they hear "1.4 million" and think scale. They picture some automated pipeline humming along, pulling data effortlessly from clean APIs.

That's not what it looks like.

What it actually looks like: staring at a North Carolina licensing website that only lets you search within a 2-mile radius, realizing you need to query all 855 zip codes individually. It looks like your scraper crashing at 3am because Texas changed a CSS class name. It looks like spending two days writing a parser for a single state because the HTML was generated by a server from 1998.

This is the work nobody posts about. It's not sexy. But it's the competitive advantage.

## Every State Is Its Own Puzzle

There are no standards for government licensing databases. Every state built their own system, at their own time, with their own technology, organized by their own logic. Each one is a fresh puzzle.

**Texas** organizes by county. 254 counties, each with its own contractor licensing records. You can't just search "Texas." You search Bexar County, then Dallas County, then Harris County, 254 times.

**Florida** organizes by license category. Electrical, plumbing, general contracting, roofing. Each category is a separate search interface on the DBPR (Department of Business and Professional Regulation) website. Different pagination. Different result formats.

**New York City** has three separate databases. The Department of Buildings has one. Consumer Affairs has another. The DOB license lookup is a third. Same city, three systems, three scrapers.

**North Carolina** has a radius-based search. You enter a zip code and a distance, and it returns contractors within that radius. There's no "show all" option. To get full coverage, I had to query every zip code in the state with a 2-mile radius. That's 855 queries just for one state, with overlap deduplication afterward.

**California's** contractor licensing board publishes a "License Master" CSV with nearly 100,000 records. That's a gold mine. Download, parse, done. But most states aren't that generous.

Some states use Socrata or ArcGIS APIs (Dallas, Chicago, Seattle, Orlando). Those are the good ones. Standardized endpoints, JSON responses, proper pagination. Maybe 20% of sources fall into this category.

The other 80% are ASP.NET forms from 2003, JavaScript-rendered SPAs that break without a real browser, or PDF-only databases that don't have a search interface at all.

Each new state means figuring out how the data is organized, what technology renders the pages, what fields are available, and what anti-scraping measures are in place. Every time, from scratch.

## The Anti-Scraping Dance

Government websites don't want to be scraped. Not because the data is secret (it's public), but because automated requests can overwhelm servers that were built to handle 50 users at a time.

**Rate limiting** is the most common defense. Hit a site with more than about 50 requests in quick succession and you're either getting blocked or getting served empty responses. The solution is pacing: random delays between requests, usually 2-5 seconds. That means a state with 10,000 pages of results takes hours, not minutes.

**IP blocking** kicks in when rate limiting isn't enough. The site sees the same IP address making hundreds of requests and blocks it. My solution: ProtonVPN rotation. Cycle through exit servers so each batch of requests comes from a different IP. It's not elegant. It works.

**CAPTCHAs** show up on the more sophisticated sites. "Click all the traffic lights." That kills automation. For these, you either find an alternative data source or accept that you're going to solve CAPTCHAs semi-manually while the scraper waits.

**Session expiration** is the sneaky one. Some sites require you to accept terms of service, which sets a session cookie. That cookie expires after 15 minutes or 100 requests. The scraper needs to detect when the session dies and restart the handshake. Missing this means your last 200 records are actually error pages that got saved as data.

The first version of most scrapers doesn't handle any of this well. You build the happy path, run it, watch it fail at record 47, figure out why, fix it, run it again, watch it fail at record 312 for a different reason. Repeat until it actually finishes.

## The Tools

**Selenium** for JavaScript-rendered sites. Most state licensing portals use JavaScript to load search results. You can't just fetch the HTML because the data isn't in the initial page load. Selenium drives a real Chrome browser, waits for the JavaScript to execute, then reads the rendered DOM. It's slow. It's resource-heavy. It's necessary for about 60% of sources.

**BeautifulSoup** for static HTML. The simpler sites serve complete HTML pages. BeautifulSoup parses the markup and lets you extract data with CSS selectors. Fast, lightweight, reliable. Used for about 30% of sources.

**Some sources need both.** Selenium navigates through search forms and pagination, then BeautifulSoup parses each results page. The browser handles the interaction. The parser handles the extraction.

**Direct HTTP requests** for APIs. Socrata and ArcGIS endpoints return JSON. No browser needed. Just requests with pagination parameters. These are the dream sources.

Every scraper outputs the same JSONL format regardless of source. One JSON object per line, standardized field names. This is the key architectural decision. The downstream pipeline (enrichment, dedup, import) doesn't care where the data came from. It all looks the same by the time it arrives.

## Phone Enrichment: The Slow Part

State licensing data usually includes business name, license type, and address. Sometimes phone. Sometimes email. Often neither.

For cold calling, you need phone numbers. That means enrichment.

The strategy: take each business name and state, search Google Business, parse the results for a phone number. Sounds simple. At scale, it's the longest part of the entire pipeline.

Six parallel workers process about 100 records per hour each. That's 600 records per hour across all workers. For a state like Connecticut with 132,000 records needing phone lookup, that's 220 hours of enrichment. Nine days of continuous running.

Hit rates vary wildly. Some states yield 85%+ phone matches because the businesses have strong Google presences. Others drop below 50% because they're small operations that never claimed a Google Business listing. The average lands somewhere around 65-70%.

Large enrichment jobs run in the background for days. I start them, check progress periodically, and deal with failures when workers crash or Google starts throttling.

## Data Cleaning: The Part Nobody Talks About

Raw scraped data is a mess. Here's what you actually deal with:

**Name normalization.** "JOHN'S PLUMBING LLC DBA JOHN SMITH" is one record. "John's Plumbing" is the business name. "John Smith" is the contact. "LLC" is the entity type. "DBA" means "doing business as." You need to parse all of that out of a single string field.

**Phone formatting.** (555) 123-4567. 555-123-4567. 5551234567. +1 555 123 4567. 555.123.4567. All the same number, all stored differently. Strip to 10 digits, validate, move on.

**Address parsing.** Some sources give you a single string: "123 Main St Ste 200 Austin TX 78701." Others split it into street, city, state, zip. Some put the suite number in the city field. Some have "TX" and others have "Texas." Normalize everything into structured components or downstream filtering breaks.

**Status filtering.** You don't want expired or suspended licenses in a lead list. But states use different terminology. "Active," "Current," "In Good Standing," "Renewed," "Provisional." Map everything to standard statuses and filter out the dead ones.

**Cross-source deduplication.** The same contractor appears in multiple databases. A general contractor in New York might be in the NYC Department of Buildings, the NY Department of State, and the NY Contractor Registry. Without dedup, your lead count is inflated and your client calls the same person twice.

The dedup pipeline exports all current leads from the production database, then checks each new import against existing records using business name, address, and phone as matching signals. One New York source had 100% overlap with existing data. Without this step, that would have been thousands of wasted records.

## The Timeline

The first scraper took about three days. Florida DBPR. It was the learning curve: understanding Selenium, handling pagination, dealing with rate limits, building the output format.

Each subsequent state got faster as patterns emerged. A new static HTML source might take half a day. A JavaScript-rendered one, a full day. A weird one like North Carolina's radius search, two days.

The entire pipeline (68 sources across 15+ states, enrichment, dedup, CRM) took about three months of building in stolen hours between my other three jobs.

Adding a new state is still a puzzle every time. But now I have the pipeline, the enrichment workers, the dedup logic, and the import scripts. The per-state cost is just writing the scraper and handling that state's specific quirks.

## Why This Matters

Anyone can say "I have 1.4 million records." The number sounds impressive in a pitch deck.

What actually matters is the pipeline. The ability to add new sources, re-scrape for fresh data, enrich phone numbers, deduplicate across databases, and deliver clean records to a CRM. The pipeline is the product, not the data.

Data goes stale. Contractors lose licenses, change phone numbers, move locations. A static database of 1.4 million records degrades every day. A pipeline that can refresh those records on demand stays valuable.

This is the moat. Not the 1.4 million number. The infrastructure that produces, cleans, and maintains the 1.4 million. Building that infrastructure required solving dozens of unglamorous problems: broken HTML, rate limits, VPN rotation, name parsing, phone lookup, dedup logic.

Nobody posts about debugging a session cookie expiration at midnight. But that's the work that makes the clean spreadsheet of leads possible. The work behind the work.
